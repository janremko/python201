{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a015b979-28ab-4b90-9dc4-b5d754a25307",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    "  table {margin-left: 0 !important;}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b60338-5ae0-4c97-8be0-35375b73b6e6",
   "metadata": {},
   "source": [
    "# Code testing some notions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f7b1e4-ee48-4e96-b4c5-5a1e84090519",
   "metadata": {},
   "source": [
    "* manual testing vs automated testing\n",
    "* unit testing vs integration testing\n",
    "* when to test\n",
    "* TDD or TFD\n",
    "* a special note on testing notebooks\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c36d45e-983b-49b4-bda0-3c33e93a3ad6",
   "metadata": {},
   "source": [
    "# Python testing with pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f8ee2f-402b-4cf8-8478-8858b05a4501",
   "metadata": {},
   "source": [
    "## 1 Getting started with pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c39c488-13ee-441b-8a59-bb78554f51b5",
   "metadata": {},
   "source": [
    "pytest is basically a wrapper around the Python native __assert__ keyword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22faff28-fadd-4d11-beb1-df5259960c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_passing():\n",
    "    assert (1, 2, 3) == (1, 2, 3)\n",
    "\n",
    "test_passing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b600d55-6218-4895-af65-638b4429042b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_failing():\n",
    "    assert (1, 2, 3) == (3, 2, 1)\n",
    "\n",
    "test_failing()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbed322-4a36-4227-9a55-136f15d2bb11",
   "metadata": {},
   "source": [
    "**Installing pytest**\n",
    "\n",
    "- ```python -m venv whatevername```\n",
    "- ```whatevername\\scripts\\activate.bat```\n",
    "- ```pip install pytest```\n",
    "\n",
    "**Running pytest**\n",
    "- ```cd /path/to/code/ch1```\n",
    "- ```pytest test_one.py```\n",
    "- ```pytest test_two.py```\n",
    "- ```pytest -v test_two.py```\n",
    "- ```pytest``` (pytest will look for all files containing tests in the current working directory)\n",
    "- ```pytest --tb=no``` (not traceback)\n",
    "- ```pytest --tb=no test_one.py, test_two.py``` (this actually does the same...)\n",
    "- ```cd ..```\n",
    "- ```pytest --tb=no ch1``` (using a path)\n",
    "- ```pytest -v ch1/test_one.py::test_passing``` (running one specific test function)\n",
    "\n",
    "**Test discovery**\n",
    "- Test files should be name ```test_<something>.py``` or ```<something>_test.py```\n",
    "- Test methods and functions should be named ```test_<something>```\n",
    "- Test classes should be named ```Test<something>```\n",
    "\n",
    "**Test Outcomes**\n",
    "- PASSED (.)\n",
    "- FAILED (F)\n",
    "- SKIPPED (s) (```@pytest.mark.skip()``` or ```@pytest.mark.skipif()``` decorators)\n",
    "- XFAIL (x) (```@pytest.mark.xfail()```)\n",
    "- XPASS (X)\n",
    "- ERROR (E)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770ec7c7-8c58-465d-a3a3-8613d11a8844",
   "metadata": {},
   "source": [
    "## 2 Writing test functions\n",
    "\n",
    "Installing the sample application\n",
    "\n",
    "- ```pip install ./code/cards_proj/```\n",
    "\n",
    "Lets play around with the cards todo list application:\n",
    "\n",
    "- ```cards add do something --owner wessel```\n",
    "- ```cards add do something else```\n",
    "- ```cards``` (retrieve a list of todos)\n",
    "- ```cards update 2 --owner wessel```\n",
    "- ```cards```\n",
    "- ```cards start 1```\n",
    "- ```cards finish 1```\n",
    "- ```cards start 2```\n",
    "- ```cards```\n",
    "- ```cards delete 1```\n",
    "- ```cards```\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3664f911-7068-41b5-aa8f-e9c55b7797ea",
   "metadata": {},
   "source": [
    "**Writing knowledge-building tests**\n",
    "\n",
    "Cards is a three layered application: CLI API and DB. The data structure used to pass infor between the CLI and the API is a class called Card:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a6768c-7e1e-47f6-ab9b-755632cd677e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "\n",
    "@dataclass\n",
    "class Card:\n",
    "    summary: str = None\n",
    "    owner: str = None\n",
    "    state: str = 'todo'\n",
    "    id: int = field(default=None, compare=False)\n",
    "\n",
    "    @classmethod\n",
    "    def from_dict(cls, d):\n",
    "        return Card(**d)\n",
    "\n",
    "    def to_dict(self):\n",
    "        return asdict(self)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659a032e-21c8-4610-a7ff-6aaab2b6bfef",
   "metadata": {},
   "source": [
    "Have a look at ```ch2\\test_card.py\n",
    "\n",
    "then run the tests:\n",
    "\n",
    "- ```pytest test_card.py```\n",
    "\n",
    "**Using assert statements**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7674be6-80c7-4295-8b89-c29369b8a532",
   "metadata": {},
   "source": [
    "| pytest | unittest |\n",
    "| :- | :- |\n",
    "| assert something | assertTrue(something) |\n",
    "| assert not something | assertFalse(something) |\n",
    "| assert a == b | assertEqual(a, b) |\n",
    "| assert a != b | assertNotEqual(a, b) |\n",
    "| assert a is None | assertIsNone(a) |\n",
    "| assert a is not None | assertIsNotNone(a) |\n",
    "| assert a <= b | assertLessEqual(a, b) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8f5862-364c-4efd-9dc5-8df4d7325868",
   "metadata": {},
   "source": [
    "show ```ch2/test_card_fail.py```\n",
    "\n",
    "then run:\n",
    "\n",
    "- ```pytest test_card_fail.py```\n",
    "\n",
    "ok, with the ```-vv``` flag we get more info:\n",
    "\n",
    "- ```pytest -vv test_card_fail.py```\n",
    "\n",
    "To see what Python itself returns when it encounters an assertion failure run\n",
    "\n",
    "- ```python test_card_fail.py```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0916b34-6a3a-4f20-b9e8-6dd12c2beb65",
   "metadata": {},
   "source": [
    "**Failing with ```pytest.fail()``` and Exceptions**\n",
    "\n",
    "A test will fail if:\n",
    "- an assert statement fails -- AssertionError\n",
    "- the test code calls ```pytest.fail()```\n",
    "- any other exception is raised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc67518-f1ed-430b-b077-99390c057209",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "\n",
    "def test_with_fail():\n",
    "    pytest.fail()\n",
    "test_with_fail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b2a250-a997-4569-8bf9-aebc199afc29",
   "metadata": {},
   "source": [
    "Let's see what that looks like from pytest, but first a look at the code:\n",
    "\n",
    "```ch2/test_alt_fail.py```\n",
    "\n",
    "Run:\n",
    "\n",
    "* ```pytest test_alt_fail.py```\n",
    "\n",
    "No assertion rewriting here, so when can this be useful then? Well, in the case of Assertion Helper Functions they might. Have a look at ```ch2/test_helper.py``` and see when this is useful. Then run te code:\n",
    "\n",
    "* ```pytest test_helper.py```\n",
    "\n",
    "So sometimes something is better than nothing..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b16054-cef4-492f-a211-e3859af2f216",
   "metadata": {},
   "source": [
    "**Testing for Expected Exceptions**\n",
    "\n",
    "Have a look at the code in ```ch2\\test_experiment.py```\n",
    "\n",
    "Then run it:\n",
    "\n",
    "* ```pytest --tb=short test_experiment.py```\n",
    "\n",
    "Using ```pytest.raises()``` we can check for the occurance of an exception. Have a look at the ```test_no_path_raises()``` function in ```ch2/test_exceptions.py```.\n",
    "\n",
    "Then run:\n",
    "\n",
    "* ```pytest test_exception.py::test_no_path_raises```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a063b2-44f7-4fd9-9bb6-e9a9b8ff99cd",
   "metadata": {},
   "source": [
    "## Structuring Test Functions\n",
    "\n",
    "The best pratices for structuring functions can be called as follows:\n",
    "\n",
    "1. Arrange - Act - Assert\n",
    "2. Given - When - Then\n",
    "\n",
    "Have a look at ```ch2\\test_structure.py```\n",
    "\n",
    "Keep in mind that a test function should only test for one behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cedc49-e53e-4b0d-97f9-55ec3147b0ec",
   "metadata": {},
   "source": [
    "**Grouping tests with classes**\n",
    "\n",
    "Have a look a ```ch2\\test_classes.py```. Then run the code:\n",
    "\n",
    "* ```pytest -v test_classes.py::TestEquality``` (the class is called)\n",
    "\n",
    "* ```pytest -v test_classes.py::TestEquality::test_equality``` (a specific method is called)\n",
    "\n",
    "All OOP features are applicable to test classes but be very reluctant to apply them!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a708cf3-6abe-4296-ac51-2a3dbee80243",
   "metadata": {},
   "source": [
    "## 3 pytest Fixtures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a0accb-3de9-4499-a732-1e9ab22a6d73",
   "metadata": {},
   "source": [
    "**<font color='red'>!!!First present the decorators notebook!!!</font>**\n",
    "\n",
    "Have a look at ```ch3\\test_fixtures.py``` and then run it:\n",
    "\n",
    "* ```pytest test_fixtures.py::test_some_data```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55c6eca-d2c6-44eb-91d4-c4932dd5c213",
   "metadata": {},
   "source": [
    "**Using fixtures for Setup and Teardown**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05df458f-d21a-4fac-b3ea-4b33f8328a13",
   "metadata": {},
   "source": [
    "Let's test the count functionality of the Card program. Have a first try with ```ch3\\test_count_initial.py``` and point to the problems.\n",
    "\n",
    "Then we have a look at ```ch3\\test_count.py```, notice the setup and teardown code and the yield in between. Also the teardown code will run whatever the outcome of the test will be.\n",
    "\n",
    "Run the following:\n",
    "\n",
    "- ```pytest test_count.py::test_empty```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede76ad9-e80b-4bd9-9807-e4c0f356ffe0",
   "metadata": {},
   "source": [
    "**Tracing Fixture Execution with --setup-show**\n",
    "\n",
    "run:\n",
    "\n",
    "```pytest --setup-show test_count.py```\n",
    "\n",
    "**Specifying Fixture Scope**\n",
    "\n",
    "Have a look at ```ch3\\test_mod_scope.py```, then run it\":\n",
    "\n",
    "* ```ytest --setup-show test_mod_scope.py```\n",
    "\n",
    "Scope can be:\n",
    "\n",
    "- function (default)\n",
    "- class\n",
    "- module\n",
    "- package\n",
    "- session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cae957-aa25-4197-a2d3-9cd91583a9a4",
   "metadata": {},
   "source": [
    "**Sharing fixtures through conftest.py**\n",
    "\n",
    "Have a look at: ```ch3\\a\\conftest.py``` and ```ch3\\a\\test_count.py``` then run:\n",
    "\n",
    "- ```cd a```\n",
    "- ```pytest --setup-show test_count.py```\n",
    "\n",
    "*Don't import conftest.py*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6d0466-b907-4173-8c2d-7ed2cb242550",
   "metadata": {},
   "source": [
    "**Finding where fixtures are defined**\n",
    "\n",
    "run: ```pytest --fixtures -v```\n",
    "\n",
    "Mmmm, that's a lot of fixtures...\n",
    "\n",
    "To be more specific use: ```pytest --fixtures-per-test test_count.py::test_empty```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9df716-3299-4606-8864-ba8447136d62",
   "metadata": {},
   "source": [
    "### Multiple fixture levels\n",
    "\n",
    "Have a look at ```ch3\\a\\test_three.py```\n",
    "\n",
    "run: ```pytest -v test_three.py```\n",
    "\n",
    "and now run ```pytest -v --tb=line```, that wasn't going to well\n",
    "\n",
    "Have a look at ```ch3\\b\\conftest.py``` for a fix\n",
    "\n",
    "run: \n",
    "\n",
    "- ```cd ..\\b```\n",
    "- ```pytest --setup-show```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53e36be-5a5c-4bf6-9f6a-973c705e252a",
   "metadata": {},
   "source": [
    "## 4 Builtin Fixtures\n",
    "\n",
    "Some setup and teardown tasks are so common that the fixtures to realize these are a standard part of pytest:\n",
    "\n",
    "- ```tmp_path``` to create a function scoped temporary directory\n",
    "- ```tmp_path_factory``` to create session scoped temporary directories\n",
    "- ```capsys``` to capture output to stdout and stderr\n",
    "- ```monkeypatch``` to modify objects, dictionaries, environment variables, cwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36aafd5a-edf4-4d08-9aed-140cc9c90d35",
   "metadata": {},
   "source": [
    "## 5 Parametrization\n",
    "\n",
    "Let's have a look at generating test by parametrizing functions and fixtures.\n",
    "\n",
    "We want to test the following method from cards\\api.py:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f170995-d654-4fe4-b2ca-6b622f8f87be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finish(self, card_id: int):\n",
    "    '''Set a card state to 'done'.'''\n",
    "    self.update_card(card_id, card(state=\"done\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c0d0ca-f8c7-46e1-a109-00a4ac7bd297",
   "metadata": {},
   "source": [
    "Have a look at the code in ```ch5\\test_finish.py``` and the run it:\n",
    "\n",
    "- ```pytest -v test_finish.py```\n",
    "\n",
    "This seems a bit verbose for three very similar tests. Let's look at ```ch5\\test_finish_combined``` for a possible solution. An then run it:\n",
    "\n",
    "- ```pytest -v test_finish_combined```\n",
    "\n",
    "Mmm, three test now look like one, difficult to understand what is going on, difficult to debug when something goes wrong. And as soon as one assert fails the remaining asserts won't be run...\n",
    "\n",
    "So let's parametrize things. Have a look at ```ch5\\test_func_param.py```, and then run it:\n",
    "\n",
    "- ```pytest -v test_func_param.py::test_finish```\n",
    "\n",
    "That went pretty well, however changing the summary for every test is redundant for this test, so we can get rid of it. Have a look at ```ch5\\test_func_param.py``` again and no at the ```test_finish_simple()``` function. And then run it:\n",
    "\n",
    "- ```pytest -v test_func_param.py::test_finish_simple```\n",
    "\n",
    "We can also parametrize at the fixture level. Have a look at ```ch5\\test_fix_param.py```. And then run it:\n",
    "\n",
    "- ```pytest -v test_fix_param.py```\n",
    "\n",
    "That looks just about the same with a bit more code. Parametrizing fixtures is useful when you need to setup and teardown stuff for each set of parameters. And you are also able to share parameters between functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15b5e46-abbc-44ce-9204-c09ba96b4749",
   "metadata": {},
   "source": [
    "## 6 Markers\n",
    "\n",
    "pytest markers are like tags or labels that tell there's something special about a specific test.\n",
    "\n",
    "```@pytest.mark.slow``` can be used to tag long running tests which need to be omitted when in a hurry.\n",
    "\n",
    "Some builtin markers modify the behavior of tests like ```@pytest.mark.parametrize``` from the previous chapter.\n",
    "\n",
    "Some builtin markers:\n",
    "- ```@pytest.mark.filterwarning```\n",
    "- ```@pytest.mark.skip```\n",
    "- ```@pytest.mark.skipif```\n",
    "- ```@pytest.mark.xfail```\n",
    "- ```@pytest.mark.parametrize```\n",
    "\n",
    "Custom markers can be handy to run only a portion of the tests. Have a look at ```ch6\\smoke\\test_start.py```, and then run it:\n",
    "\n",
    "- ```pytest -v -m smoke test_start.py```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87218c49-560f-4747-b540-cbe02b463d2e",
   "metadata": {},
   "source": [
    "## 7 Strategy\n",
    "\n",
    "### Determining Test Scope\n",
    "\n",
    "Different projects have different test goals and requirements\n",
    "- Security\n",
    "- Performance\n",
    "- Workload (scaling)\n",
    "- Input validation\n",
    "\n",
    "Testing Enough to sleep at night"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172f77aa-1f7b-4a8a-b2a5-230c97ca9f88",
   "metadata": {},
   "source": [
    "### Evaluating and prioritizing the features to test\n",
    "\n",
    "- Recent\n",
    "- Core or USPs\n",
    "- Risk\n",
    "- Problematic\n",
    "- Expertise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e27423e-1610-4412-8d9d-739bac7bb193",
   "metadata": {},
   "source": [
    "## 8 Configuration files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc1c5dd-164a-4822-8d44-e9f4f0cdd800",
   "metadata": {},
   "source": [
    "## 9 coverage\n",
    "\n",
    "Coverage is a measure of how much of your code is being covered by tests. **Line coverage** is calculated by dividing the number of lines of code touched by tests by the total number of lines of code.\n",
    "**Branch coverage** tells you wether all paths through the code are taken or not.\n",
    "\n",
    "Keep in mind that coverage doesn't tell you anyting about test quality...\n",
    "\n",
    "In Python ```Coverage.py``` is the prefered tool to measure code coverage. ```pytest-cov``` is a pytest pluging which makes working with ```coverage.py``` in pytest easier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ab6745-49e3-498b-8495-b959cf365fd1",
   "metadata": {},
   "source": [
    "## 10 Mocking\n",
    "\n",
    "Mocking means immitating parts of the system for testing purposes. A unit under test might have dependecies on other (complex) objects. To isolate the behavior of the unit under test you replace the other parts by mocks that simulate the behavior of the real objects. You can also look at it as a crash test dummy.\n",
    "\n",
    "Within Python the ```mock``` package is used for this purpose. It ships with the standard library as ```unittest.mock```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c446e5f-fc19-48d5-9f11-0e1f1697bb22",
   "metadata": {},
   "source": [
    "## 11 Continuous integration\n",
    "\n",
    "CI is the automated merging building and testing of code which is committed to a repository.\n",
    "\n",
    "On a local scale we can use the tox command-line tool to implement a CI like way of working. We can define different envirnoments for which the complete test suite is run whenever we want.\n",
    "\n",
    "tox can also be integrated in an online CI tool like GitHub Actions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73b0a7d-1150-4cc5-b27c-2a1fcbb39f8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
